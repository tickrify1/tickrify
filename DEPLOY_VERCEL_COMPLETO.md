# ğŸš€ DEPLOY COMPLETO NA VERCEL

Guia para fazer deploy de **TUDO** (Frontend + Backend + Worker) na Vercel.

---

## âš ï¸ IMPORTANTE: LIMITAÃ‡Ã•ES DA VERCEL

### O que funciona bem:
âœ… **Frontend** - Perfeito, CDN global, super rÃ¡pido  
âœ… **Backend API** - Funciona como Serverless Functions  
âœ… **Database** - Supabase externo funciona perfeitamente  

### âš ï¸ LIMITAÃ‡ÃƒO CRÃTICA: Worker AI
âŒ **Worker nÃ£o pode rodar continuamente na Vercel**

**Por quÃª?**
- Vercel usa **Serverless Functions** (executam sob demanda)
- Functions tÃªm timeout mÃ¡ximo de **300 segundos (5 minutos)** no plano Pro
- No plano Hobby: **10 segundos apenas**
- Worker precisa rodar **continuamente** para processar jobs

### ğŸ”„ SOLUÃ‡ÃƒO HÃBRIDA

**OpÃ§Ã£o A: Vercel + ServiÃ§o Externo para Worker**
- Frontend + Backend â†’ Vercel
- Worker â†’ Railway/Render (gratuito)
- Redis â†’ Upstash (gratuito)

**OpÃ§Ã£o B: Modo SÃ­ncrono (sem Worker)**
- AnÃ¡lise processada diretamente na API
- Sem fila, anÃ¡lise instantÃ¢nea
- âš ï¸ Timeout de 10s no Hobby, 300s no Pro

---

## ğŸ¯ OPÃ‡ÃƒO A: VERCEL + WORKER EXTERNO (RECOMENDADO)

### Arquitetura
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              VERCEL                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   FRONTEND   â”‚â”€â”€â”€â”€â–¶â”‚   BACKEND    â”‚         â”‚
â”‚  â”‚              â”‚     â”‚ (Serverless) â”‚â”€â”€â”€â”€â”€â”   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚  Redis   â”‚
                                        â”‚ (Upstash)â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚  WORKER  â”‚
                                        â”‚(Railway) â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Passo 1: Preparar Monorepo para Vercel

A Vercel precisa de configuraÃ§Ã£o especÃ­fica para monorepos:

```bash
cd /Users/vini.mqs/Documents/tickrify_novo
```

### Passo 2: Criar vercel.json na raiz

JÃ¡ existe, mas vamos otimizÃ¡-lo:

```json
{
  "version": 2,
  "builds": [
    {
      "src": "apps/frontend/package.json",
      "use": "@vercel/static-build",
      "config": {
        "distDir": "apps/frontend/dist"
      }
    },
    {
      "src": "apps/backend/src/vercel.ts",
      "use": "@vercel/node",
      "config": {
        "maxLambdaSize": "50mb",
        "includeFiles": ["apps/backend/prisma/**"]
      }
    }
  ],
  "routes": [
    {
      "src": "/api/(.*)",
      "dest": "apps/backend/src/vercel.ts"
    },
    {
      "handle": "filesystem"
    },
    {
      "src": "/(.*)",
      "dest": "apps/frontend/dist/$1"
    }
  ],
  "env": {
    "DATABASE_URL": "@database_url",
    "CLERK_PUBLISHABLE_KEY": "@clerk_publishable_key",
    "CLERK_SECRET_KEY": "@clerk_secret_key",
    "OPENAI_API_KEY": "@openai_api_key",
    "REDIS_URL": "@redis_url",
    "USE_LOCAL_STORAGE": "false",
    "NODE_ENV": "production"
  }
}
```

### Passo 3: Deploy na Vercel

**Via Dashboard:**

1. Acesse: https://vercel.com/
2. **"Add New" â†’ "Project"**
3. Escolha `tickrify-novo`
4. Configure:
   - **Framework Preset**: Other
   - **Root Directory**: `./` (raiz)
   - **Build Command**: 
     ```bash
     cd apps/frontend && npm install && npm run build && cd ../backend && npm install && npm run build && npx prisma generate
     ```
   - **Output Directory**: `apps/frontend/dist`

5. **Environment Variables** (adicione todas):
   ```
   DATABASE_URL=postgresql://...
   CLERK_PUBLISHABLE_KEY=pk_test_...
   CLERK_SECRET_KEY=sk_test_...
   OPENAI_API_KEY=sk-proj-...
   REDIS_URL=redis://...
   FRONTEND_URL=https://seu-app.vercel.app
   USE_LOCAL_STORAGE=false
   NODE_ENV=production
   ```

6. Deploy!

### Passo 4: Setup Redis (Upstash - Gratuito)

1. Acesse: https://upstash.com/
2. Crie conta
3. **"Create Database"** â†’ Redis
4. **Region**: Escolha mais prÃ³ximo
5. Copie **UPSTASH_REDIS_REST_URL**
6. Adicione como `REDIS_URL` na Vercel

### Passo 5: Deploy Worker no Railway (Gratuito)

1. Acesse: https://railway.app/
2. **"New Project" â†’ "Empty Project"**
3. **"+ New" â†’ "GitHub Repo"**
4. Escolha `tickrify-novo`
5. Configure:
   - **Root Directory**: `apps/backend`
   - **Build Command**: `npm install && npm run build && npx prisma generate`
   - **Start Command**: `npm run worker`
   - **Variables**: Mesmas da Vercel (DATABASE_URL, REDIS_URL, etc)

Pronto! Agora vocÃª tem:
- âœ… Frontend + Backend na Vercel
- âœ… Worker no Railway (gratuito)
- âœ… Redis no Upstash (gratuito)

---

## ğŸ”¥ OPÃ‡ÃƒO B: VERCEL 100% (MODO SÃNCRONO)

### âš ï¸ LimitaÃ§Ãµes:
- Sem fila assÃ­ncrona
- AnÃ¡lise processa durante a request
- Timeout de 10s (Hobby) ou 300s (Pro)
- OpenAI geralmente responde em 10-30s
- **NÃ£o recomendado para produÃ§Ã£o sÃ©ria**

### ModificaÃ§Ãµes NecessÃ¡rias

#### 1. Modificar Backend para Modo SÃ­ncrono

Criar `apps/backend/src/modules/ai/ai-sync.service.ts`:

```typescript
import { Injectable } from '@nestjs/common';
import { PrismaService } from '../database/prisma.service';
import OpenAI from 'openai';
import TRADING_SYSTEM_PROMPT from '../../common/prompts/trading-system-prompt';

@Injectable()
export class AiSyncService {
  private openai: OpenAI;

  constructor(private prisma: PrismaService) {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });
  }

  async analyzeSynchronously(
    userId: string,
    imageBase64: string,
  ) {
    try {
      // Criar anÃ¡lise no banco
      const analysis = await this.prisma.analysis.create({
        data: {
          userId,
          imageUrl: imageBase64,
          status: 'processing',
        },
      });

      // Chamar OpenAI diretamente
      const response = await this.openai.chat.completions.create({
        model: 'gpt-4-vision-preview',
        max_tokens: 4000,
        messages: [
          {
            role: 'system',
            content: TRADING_SYSTEM_PROMPT,
          },
          {
            role: 'user',
            content: [
              {
                type: 'text',
                text: 'Analyze this trading chart and provide detailed analysis in JSON format.',
              },
              {
                type: 'image_url',
                image_url: {
                  url: imageBase64,
                },
              },
            ],
          },
        ],
      });

      const content = response.choices[0]?.message?.content || '';
      
      // Parse resposta
      let cleanContent = content.trim();
      if (cleanContent.startsWith('```json')) {
        cleanContent = cleanContent.replace(/```json\n?/g, '').replace(/```\n?/g, '');
      }

      const parsed = JSON.parse(cleanContent);

      // Atualizar anÃ¡lise
      await this.prisma.analysis.update({
        where: { id: analysis.id },
        data: {
          status: 'done',
          recommendation: parsed.recommendation,
          confidence: parsed.confidence,
          reasoning: parsed.reasoning,
          fullResponse: parsed,
        },
      });

      return {
        id: analysis.id,
        status: 'done',
        recommendation: parsed.recommendation,
        confidence: parsed.confidence,
        analysis: parsed.analysis,
      };
    } catch (error) {
      console.error('[AiSync] Error:', error);
      throw error;
    }
  }
}
```

#### 2. Atualizar Controller

```typescript
// Adicionar no ai.controller.ts

@Post('analyze-sync')
@UseInterceptors(FileInterceptor('file'))
async analyzeSynchronous(
  @UploadedFile() file: Express.Multer.File,
  @CurrentUser() user: any,
) {
  const imageBase64 = `data:${file.mimetype};base64,${file.buffer.toString('base64')}`;
  
  return this.aiSyncService.analyzeSynchronously(user.id, imageBase64);
}
```

#### 3. Atualizar Frontend

```typescript
// apps/frontend/src/lib/api.ts

export async function createAnalysisSync(file: File): Promise<AIAnalysisResponse> {
  const formData = new FormData();
  formData.append('file', file);

  const response = await fetch(`${API_BASE_URL}/api/ai/analyze-sync`, {
    method: 'POST',
    headers: {
      Authorization: `Bearer ${await getToken()}`,
    },
    body: formData,
  });

  if (!response.ok) {
    throw new Error('Failed to analyze');
  }

  return response.json();
}
```

### âš ï¸ Problemas do Modo SÃ­ncrono:

1. **Timeout**: Se OpenAI demorar >10s, vai falhar no plano Hobby
2. **Sem Retry**: Se falhar, usuÃ¡rio precisa fazer upload novamente
3. **Sem Fila**: NÃ£o hÃ¡ controle de concorrÃªncia
4. **Cold Starts**: Primeira request pode demorar muito
5. **ExperiÃªncia Ruim**: UsuÃ¡rio fica esperando na tela

---

## ğŸ“Š COMPARAÃ‡ÃƒO DAS OPÃ‡Ã•ES

| CaracterÃ­stica | Vercel + Railway Worker | Vercel 100% SÃ­ncrono |
|----------------|------------------------|----------------------|
| **Custo** | Vercel (grÃ¡tis) + Railway ($0-5) | Vercel (grÃ¡tis ou $20 Pro) |
| **Complexidade** | MÃ©dia (2 plataformas) | Baixa (1 plataforma) |
| **Confiabilidade** | âœ… Alta | âš ï¸ MÃ©dia (timeouts) |
| **Performance** | âœ… AssÃ­ncrono, fila | âŒ Bloqueante |
| **Escalabilidade** | âœ… Excelente | âš ï¸ Limitada |
| **Timeout** | âœ… Sem limite | âŒ 10s (Hobby) ou 300s (Pro) |
| **Retry** | âœ… AutomÃ¡tico | âŒ Manual |
| **UX** | âœ… Polling, nÃ£o-bloqueante | âŒ UsuÃ¡rio espera |
| **Recomendado** | âœ… SIM | âŒ NÃ£o para produÃ§Ã£o |

---

## ğŸ¯ MINHA RECOMENDAÃ‡ÃƒO

### **OpÃ§Ã£o A: Vercel + Railway Worker** (Recomendado)

**Por quÃª?**
- âœ… Melhor experiÃªncia do usuÃ¡rio
- âœ… Mais confiÃ¡vel (sem timeouts)
- âœ… EscalÃ¡vel
- âœ… Railway tem $5 gratuito/mÃªs (suficiente para comeÃ§ar)
- âœ… FÃ¡cil de configurar

**Custo Total:**
- Vercel: $0/mÃªs
- Railway: $0-5/mÃªs
- Upstash Redis: $0/mÃªs
- **TOTAL: $0-5/mÃªs**

### Passo a Passo Simplificado:

1. **Deploy Frontend + Backend na Vercel** (10 min)
2. **Setup Redis no Upstash** (5 min)
3. **Deploy Worker no Railway** (10 min)

**Tempo total: ~25 minutos**

---

## ğŸš€ SCRIPT DE DEPLOY VERCEL COMPLETO

Vou criar um script automatizado:

```bash
#!/bin/bash

echo "ğŸš€ DEPLOY VERCEL + RAILWAY"
echo "=========================="
echo ""

# 1. Setup Upstash Redis
echo "ğŸ“¦ Passo 1: Setup Redis (Upstash)"
echo "   1. Acesse: https://upstash.com/"
echo "   2. Crie database Redis"
echo "   3. Copie REDIS_URL"
read -p "   Cole o REDIS_URL aqui: " REDIS_URL

# 2. Deploy na Vercel
echo ""
echo "â–² Passo 2: Deploy na Vercel"
cd apps/frontend
vercel --prod

echo ""
echo "URL do app:"
read -p "Cole a URL do Vercel aqui: " VERCEL_URL

# 3. Configurar variÃ¡veis na Vercel
echo ""
echo "âš™ï¸  Passo 3: Adicionar variÃ¡veis na Vercel"
echo "   Acesse: https://vercel.com/dashboard"
echo "   Settings â†’ Environment Variables"
echo ""
echo "Adicione:"
echo "  REDIS_URL=$REDIS_URL"
echo "  (+ outras variÃ¡veis)"
read -p "Pressione Enter quando terminar..."

# 4. Deploy Worker no Railway
echo ""
echo "ğŸš‚ Passo 4: Deploy Worker no Railway"
echo "   1. Acesse: https://railway.app/"
echo "   2. New Project â†’ GitHub Repo"
echo "   3. Root: apps/backend"
echo "   4. Start: npm run worker"
echo "   5. Adicione as mesmas variÃ¡veis"
read -p "Pressione Enter quando terminar..."

echo ""
echo "âœ… DEPLOY COMPLETO!"
echo ""
echo "URLs:"
echo "  Frontend: $VERCEL_URL"
echo "  Backend:  $VERCEL_URL/api"
echo "  Worker:   Railway"
```

---

## ğŸ’¡ ALTERNATIVA: VERCEL PRO + MODO SÃNCRONO

Se vocÃª tem **Vercel Pro** ($20/mÃªs):
- Timeout de 300s (5 minutos)
- OpenAI geralmente responde em 10-30s
- Funcionaria, mas ainda nÃ£o Ã© ideal
- Sem retry automÃ¡tico
- Sem controle de fila

**Veredito**: Mesmo com Pro, Worker externo Ã© melhor.

---

## âœ… CONCLUSÃƒO

**Melhor opÃ§Ã£o**: Vercel (Frontend + Backend) + Railway (Worker)

**Por quÃª?**
1. âœ… Gratuito ou $5/mÃªs
2. âœ… ConfiÃ¡vel (sem timeouts)
3. âœ… Melhor UX (assÃ­ncrono)
4. âœ… EscalÃ¡vel
5. âœ… FÃ¡cil de configurar

**Vercel 100% sÃ³ funciona bem se:**
- VocÃª tem Vercel Pro ($20/mÃªs)
- AceitÃ¡vel ter timeouts ocasionais
- NÃ£o precisa de alta escala
- ProtÃ³tipo/MVP rÃ¡pido

---

## ğŸ“ PRÃ“XIMOS PASSOS

### Para Vercel + Railway (Recomendado):

```bash
# 1. Ler guia
cat DEPLOY_RAPIDO.md

# 2. Deploy frontend na Vercel
cd apps/frontend
vercel --prod

# 3. Setup Redis (Upstash)
# https://upstash.com/

# 4. Deploy worker (Railway)
# https://railway.app/
# Siga DEPLOY_RAPIDO.md seÃ§Ã£o Railway
```

### Para Vercel 100% (Modo SÃ­ncrono):

1. Implementar `ai-sync.service.ts` (cÃ³digo acima)
2. Atualizar controller e frontend
3. Deploy na Vercel
4. âš ï¸ Testar timeout limits
5. âš ï¸ Monitorar falhas

---

**ğŸ¯ EU RECOMENDO: VERCEL + RAILWAY**

Ã‰ a melhor combinaÃ§Ã£o de custo, performance e confiabilidade!

Quer que eu crie um script automatizado para essa opÃ§Ã£o?

